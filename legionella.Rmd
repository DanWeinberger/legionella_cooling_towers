---
title: "Legionella Program"
author: "Mi Zhou"
date: "2022/4/6"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

## Data Preprocessing

The real raw data isn't included in this file. All data in ./Data has been pre-processed,  including keeping all recording date in date_of_record.txt files by year, changing the name of each csv file to the recording date, and keeping only the following variables: equipment_location_city, equipment_location_zip, equipment_specification_id, equipment_intended_use, equipment_cooling_capacity, equipment_commissioned_date, equipment_last_inspection, equipment_last_bact_sample_collection_date, equipment_last_legionella_sample_collection_date, legionella_exceedance_recode_text, equipment_latitude, equipment_longitude. 

However, "equipment_latitude" and "equipment_longitude" aren't included in all csv files because they are missing in some years. We also filter in advance to make sure there is no missing in "equipment__specification_id" and "equipment_location_zip".

## Date Integration

The preliminary result of data integration has been generated by python script in advance and is kept as ./Temp/data_integrate.txt. So, unless changes are made to data, it is unnecessary to run the python script again (and takes a really long time due to sorting process, but it can be optimized). Other data_2017.txt, data_2018.txt, data_2019.txt in ./Temp are results of data integration by year.

```{r,eval = FALSE}
#library(reticulate)
#source_python("F:/Spring 2022/EMD563 Lab and field studies/GitHub/Python/data_integrate_by_year.py")
```

Due to the problem of different formats for recording dates, which is hard to identify by Python, we load data_integrate.txt into Excel and it will automatically unified the date format. We can do a second round of screening to remove the repeating records. The result is also generated in advance and kept as ./Temp/data_integrate2.txt and ./Temp/data_integrate2.csv. Each line represents a different record of a cooling tower. 

It is also unnecessary to run this python script, but all relevant Python scripts are kept in ./Python for query. If we need to rerun any of the python script, remember to change the working directory in the script, and rerunning will overwrite previous results.

```{r,eval = FALSE}
#library(reticulate)
#source_python("F:/Spring 2022/EMD563 Lab and field studies/GitHub/Python/remove_repeating_records.py")
```

## Data preparation

```{r setup,include=FALSE,warning=FALSE}
library(ggplot2)
library(gridExtra)
library(dplyr)
library(zipcodeR)
library(lubridate)
library(scales)
```

### Load integrated data from the file

The integrated data is loaded from "./Temp/data_integrate2.csv" generated by previous steps. We keep all the records after 2015-01-01 and get information of county and state from package "zipcodeR". The process will take some time.

```{r, include = FALSE}
data=read.csv("./Temp/data_integrate2.csv")

colnames(data)=c("id","city","zip","use","commission_date","last_legionella_date","exceedance")

data$commission_date=as.Date(data$commission_date)

data$last_legionella_date=as.Date(data$last_legionella_date)

data= dplyr::filter(data,last_legionella_date>"2015-01-01")

lookup <- reverse_zipcode(data$zip) # creates a table mapping ZIPs to county/town


data <- merge(data, lookup,by.x='zip', by.y='zipcode', all.x=T)


legionella_pos=filter(data,exceedance=="yes")

legionella_neg=filter(data,exceedance=="no")
```

### Generate time series data

1. Monthly sampling number, how many samples are collected every month, labeled as m_sampling_freq in our time series data set.

```{r,echo = FALSE}
Date.bymonth=cut(data$last_legionella_date,breaks="months")
bDates=as.data.frame(table(Date.bymonth))
bDates$Date.bymonth <- as.Date(bDates$Date.bymonth)
t_dataset=bDates
colnames(t_dataset)[2]="m_sampling_freq"

data$monthdate <- floor_date(data$last_legionella_date, 'month')

month.summary <- data %>%
  group_by(monthdate) %>%
  summarize(N_test=n() , N_pos= sum(exceedance=="yes") ) %>%
  ungroup() %>%
  mutate(prop_pos=N_pos/N_test)
```

```{r,echo = FALSE, fig.height=4,fig.width=6}
ggplot(month.summary, aes(x=monthdate, y=N_test,group=1))+
  geom_line()+
  #geom_point(size=4,shape=20)+
  ggtitle("Monthly Sampling Number")+
  theme_classic()+
  ylab("Frequency")+
  xlab("Date by month")+
  theme(axis.text.x = element_text(angle=90, hjust=0.5, vjust=0.5))
  #geom_text(aes(label=Freq),hjust = 1,vjust=-0.5)
```

2. Monthly number of positive counts, how many positive records of legionella exceedance are taken every month, labeled as m_positive_freq.


```{r,echo = FALSE, fig.height=4,fig.width=6}
ggplot(month.summary, aes(x=monthdate, y=N_pos,group=1))+
  geom_line()+
  #geom_point(size=4,shape=20)+
  ggtitle("Monthly Number Positive")+
  theme_classic()+
  ylab("Frequency")+
  xlab("Date by month")+
  theme(axis.text.x = element_text(angle=90, hjust=0.5, vjust=0.5))
  #geom_text(aes(label=Freq),hjust = 1,vjust=-0.5)
```



3. Monthly proportion of positive counts, monthly positive counts/monthly sampling number, labeled as named as m_positive_p. m_positive_percentage is the percentage format of m_positive_p.
```{r,echo = FALSE, fig.height=4,fig.width=6}
ggplot(month.summary, aes(x=monthdate, y=prop_pos,group=1))+
  geom_line()+
  #geom_point(size=4,shape=20)+
  ggtitle("Proportion Positive")+
  theme_classic()+
  ylab("Frequency")+
  xlab("Date by month")+
  theme(axis.text.x = element_text(angle=90, hjust=0.5, vjust=0.5))
  #geom_text(aes(label=Freq),hjust = 1,vjust=-0.5)
```



4. Number of new towers taken into study every month, labeled as m_new_towers.

```{r,echo = FALSE}
unique=data[which(duplicated(data[,1])==FALSE),]
new_bymonth=as.data.frame(table(cut(unique$last_legionella_date,breaks="months")))
for (i in t_dataset[,1]){
  id_new=which(new_bymonth[,1]==i)
  id_t=which(t_dataset[,1]==i)
  if (length(id_new)==0){
    t_dataset[id_t,6]=0
  }
  else{
    t_dataset[id_t,6]=new_bymonth[id_new,2]
  }
}
colnames(t_dataset)[6]="m_new_towers"
```

```{r,echo = FALSE, fig.height=4,fig.width=6}
ggplot(t_dataset, aes(x=Date.bymonth, y=m_new_towers,group=1))+
  geom_line()+geom_point(size=4,shape=20)+
  ggtitle("Number of new towers taken into study monthly")+
  theme_classic()+ylab("Freq")+xlab("Date by month")+
  theme(axis.text.x = element_text(angle=90, hjust=0.5, vjust=0.5))+
  geom_text(aes(label=m_new_towers),hjust = 1,vjust=-0.5)
```

### Other descriptive analysis 

1. Median and mean sampling interval. Sampling intervals are calculated separately for each equipment with 2 or more records. Then we get the median and mean for overall sampling intervals.

```{r,echo = FALSE}
sampling_interval=data.frame()
for (i in 2:length(data[,1])){
  if (data[i,1]==data[i-1,1]){
    interval=as.numeric(as.Date(data[i,6])-as.Date(data[i-1,6]))
    sampling_interval=rbind(sampling_interval,c(data[i,1],interval))
  }
}
colnames(sampling_interval)=c("id","interval")
```

```{r}
mean(as.numeric(sampling_interval[,2])) #135.6387 days
median(as.numeric(sampling_interval[,2])) #88 days
```

2. Median and mean interval from positive to negative, the time that an equipment needs to reduce its legionella level to the level lower than the threshold. However, the result is largely affected by sampling intervals. With more data, the result may be more reliable.

```{r,echo=FALSE}
pos_neg_interval=data.frame()
first_pos=legionella_pos[which(duplicated(legionella_pos[,1])==FALSE),]
for (i in 1:length(first_pos[,1])){
  pos_id=first_pos[i,1]
  pos_date=first_pos[i,6]
  first_neg_after_pos=which(data$id==pos_id & 
                              data$last_legionella_date>pos_date & 
                              data$exceedance=="no")[1]
  neg_date=data[first_neg_after_pos,6]
  interval=as.numeric(as.Date(neg_date)-as.Date(pos_date))
  pos_neg_interval=rbind(pos_neg_interval,c(pos_id,interval))
}
```

```{r}
mean(na.omit(pos_neg_interval[,2])) #88.28049
median(na.omit(pos_neg_interval[,2])) #22
```

3. Positive counts by counties, the number of positive records in each county. But New York County has the largest number of positive records probably just because more samples were taken in New York County. If we also count how many samples were taken in each county, we may be able to adjust the result to see if there is a regional tendency.

```{r,echo=FALSE}
pos_county=as.data.frame(table(first_pos$county))
colnames(pos_county)[1]="County"
```

```{r,echo=FALSE,fig.height=6,fig.width=6}
ggplot(pos_county, aes(x=County, y=Freq,group=1))+
  geom_bar(stat="identity")+
  ggtitle("Positive counts by counties")+
  theme_classic()+ylab("Freq")+xlab("County")+
  theme(axis.text.x = element_text(angle=90, hjust=0.5, vjust=0.5))+
  geom_text(aes(label=Freq),hjust = 1,vjust=-0.5)
```

4. Age of cooling towers with positive records. The results suggest an average service time of a cooling tower before it becomes easier to be polluted by legionell.

```{r,echo=FALSE}
first_pos[,10]=as.numeric(as.Date(first_pos$last_legionella_date)-as.Date(first_pos$commission_date))/365
colnames(first_pos)[10]=c("Service age")
```

```{r}
mean(na.omit(first_pos$`Service age`)) #12.489
median(na.omit(first_pos$`Service age`)) #11.893
```

## Harmonic regression

```{r, include = FALSE}
time=c((2015*12):(2019*12+10))
```

1. Plot of number of positive cases vs time, and proportion of positive cases vs time.

```{r,echo = FALSE, fig.height=3,fig.width=4}
plot(x=time/12,y=t_dataset$m_positive_freq,type="l",xlab="Year",
     ylab="Positive cases",col="blue",bty="l")
```

```{r,echo = FALSE, fig.height=3,fig.width=4}
plot(x=time/12,y=t_dataset$m_positive_p*100,type="l",xlab="Year",
     ylab="Percentage of positive cases (%)",col="blue",bty="l")
```

2. Construct the model with frequency(number of positive cases).
Get monthly positive cases (in log scale), labeled as logpos. To avoid 0 causing log(0)=-Inf, we simply add 1 to each item before log transformation.

```{r,echo = FALSE}
logpos=log(t_dataset$m_positive_freq+1)
```

```{r,echo = FALSE, fig.height=3,fig.width=4}
plot(x=time/12,y=logpos,type="l",xlab="Year",
     ylab="Positive cases (in log scale)",
     col="blue")
```

3. Get the dominant period for the oscillations.

```{r}
# tmax: Number of months
tmax <- length(time) 
# dt: time step in years
dt <- 1/12
# Calculate the absolute value of the power from FFT 
# using a function fft()
ym <- abs(fft(logpos))
# Frequency of oscillations (in months)
f <- (0:(tmax-1))/tmax 
# Period of oscillations (in years) = 1 / frequency
Period <- dt/f
```

```{r,fig.height=3,fig.width=4}
# Make a plot
plot(x=Period[2:round(tmax/2)+1], y=ym[2:round(tmax/2)+1], 
     type='h', xlim=c(0,4),bty="l",
     xlab='Period (years)', ylab='Strength', 
     main="Absolute value of the power from FFT")
```

Choose the first two period with largest power to be the dominant period for the oscillation.

```{r}
# What is the dominant period for the oscillations?
# About 12 months (1 year) and 6 months (0.5 year)
order(ym[2:round(tmax/2)+1], decreasing = T)[1:2] # [1]  4 9
Period[2:round(tmax/2)+1][4] # About 1 year
Period[2:round(tmax/2)+1][9] # About 0.5 year
```

4. Create harmonic terms

```{r}
# 12-month periods
cos12 <- cos(2*pi*time/12)
sin12 <- sin(2*pi*time/12)
# 6-month periods
cos6 <- cos(2*2*pi*time/12) 
sin6 <- sin(2*2*pi*time/12)
```

5.Fit harmonic regression, adjust with Poisson regression to eliminate the bias from sampling frequency in different months.log_N_tests represents the log transformation of monthly sampling number. Adding 1 to each item to avoid log(0).

```{r}
log_N_tests=log(t_dataset$m_sampling_freq+1)
mod2 <- glm(t_dataset$m_positive_freq ~ sin12 +cos12 +sin6 +cos6 +offset(log_N_tests), family='poisson')
summary(mod2)
sum(residuals(mod2,type = "pearson")^2)/mod2$df.residual
```

6. Overdispersion diagnosis. Possion model has the assumption that the expected value is equal to variance, we need to test for overdispersion. If the dispersion parameter is significantly larger than 1, we need to switch to negative binomial regression.

```{r}
# Estimation of the dispersion parameter
sum(residuals(mod2,type = "pearson")^2)/mod2$df.residual
```

We can also apply a significance test directly on the fitted model to check the overdispersion.

```{r}
library(AER)
dispersiontest(mod2)
```

If we use significance level=0.05, we get the result that we reject null hypothesis. True dispersion is greater than 1.

7. To address the overdispersion in the model, we can change our distributional assumption to the Negative binomial in which the variance is larger than the mean.

```{r}
library(MASS)
mod3 <- glm.nb(t_dataset$m_positive_freq ~ sin12 +cos12 +sin6 +cos6 +offset(log_N_tests))
summary(mod3)
```

```{r}
# Estimation of dispersion parameter
sum(residuals(mod3,type = "pearson")^2)/mod3$df.residual
```

It is a better fit to the data because the ratio of deviance over degrees of freedom is only slightly larger than 1 here.

8. Plot the prediction with the observed data in log scale.

(1) Poisson regression.

```{r, echo=FALSE}
pred <- predict(mod2, type='link')
```
```{r, echo=FALSE,fig.height=3,fig.width=4}
plot(y=pred, x=time/12, type='l', 
     col="red", ylab='Log(positive cases)', 
     xlab='Year',bty="l",
     main="Observed vs. predicted number of positive cases (Poisson)")
lines(y=logpos,x=time/12,col='blue')
legend(x="topright",
       legend=c('Observed','Predicted'),
       col=c('blue','red'),lwd=1,bty='n',cex=0.5)
```

(2) Negative binomial regression.

```{r, echo=FALSE}
pred2 <- predict(mod3, type='link')
```
```{r, echo=FALSE,fig.height=3,fig.width=4}
plot(y=pred2, x=time/12, type='l', 
     col="red", ylab='Log(positive cases)', 
     xlab='Year',bty="l",
     main="Observed vs. predicted number of positive cases (Negative Binomial)")
lines(y=logpos,x=time/12,col='blue')
legend(x="topright",
       legend=c('Observed','Predicted'),
       col=c('blue','red'),lwd=1,bty='n',cex=0.5)
```



